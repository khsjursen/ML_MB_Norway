{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad554804-cec2-427e-9c7e-ff29eda77f78",
   "metadata": {},
   "source": [
    "# ML MB model with custom loss function for predicting MB on monthly resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b38de4-4e0f-4f09-b5ee-bdcc962a54a4",
   "metadata": {},
   "source": [
    "Based on tests performed in notebook test_custom_loss.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f9d7045-596d-473d-9df8-89bbec2d3d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from plotting_functions import plot_prediction\n",
    "from plotting_functions import plot_gsearch_results\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import product\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from plotting_functions import plot_prediction_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a0dc386-ba1c-4f11-86f9-63abd19e6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for data processing\n",
    "\n",
    "# Reshape dataframe to monthly resolution\n",
    "def reshape_dataset_monthly(df, id_vars, variables, months_order): \n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for var in variables:\n",
    "        # Filter columns for the current variable and the ID columns\n",
    "        cols = [col for col in df.columns if col.startswith(var) or col in id_vars]\n",
    "        df_var = df[cols]\n",
    "\n",
    "        # Rename the columns to have just the month\n",
    "        df_var = df_var.rename(columns=lambda x: x.split('_')[-1] if x not in id_vars else x)\n",
    "\n",
    "        # Melt the DataFrame to long format and add month order\n",
    "        df_melted = pd.melt(df_var, id_vars=id_vars, var_name='month', value_name=var)\n",
    "        df_melted['month'] = pd.Categorical(df_melted['month'], categories=months_order, ordered=True)\n",
    "\n",
    "        df_list.append(df_melted)\n",
    "\n",
    "    # Combine all reshaped DataFrames\n",
    "    df_final = df_list[0]\n",
    "    for df_temp in df_list[1:]:\n",
    "        df_final = pd.merge(df_final, df_temp, on=id_vars + ['month'], how='left')\n",
    "\n",
    "    # Sort the DataFrame based on ID variables and month\n",
    "    df_final = df_final.sort_values(by=id_vars + ['month'])\n",
    "\n",
    "    return(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb806d-7159-4ded-887a-c8f673e23e7f",
   "metadata": {},
   "source": [
    "## 1 Prepare training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d140dda-15a9-4f9e-9517-66a7624833ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kasj\\AppData\\Local\\Temp\\ipykernel_18052\\398696404.py:12: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(filepath + filename)\n"
     ]
    }
   ],
   "source": [
    "# Specify filepaths and filenames.          \n",
    "loc = 'local'\n",
    "\n",
    "if loc == 'cryocloud':\n",
    "    filepath = '/home/jovyan/ML_MB_Norway_data/'\n",
    "elif loc == 'local':\n",
    "    filepath = 'Data/'\n",
    "\n",
    "filename = '2023-08-28_stake_mb_norway_cleaned_ids_latlon_wattributes_climate.csv'\n",
    "\n",
    "# Load data.\n",
    "data = pd.read_csv(filepath + filename)\n",
    "\n",
    "# Add year column\n",
    "data['year']=pd.to_datetime(data['dt_curr_year_max_date'].astype('string'), format=\"%d.%m.%Y %H:%M\")\n",
    "data['year'] = data.year.dt.year.astype('Int64')\n",
    "\n",
    "# Remove cells with nan in balance_netto.\n",
    "glacier_data_annual = data[data['balance_netto'].notna()]\n",
    "glacier_data_annual.reset_index(drop=True, inplace=True)\n",
    "\n",
    "glacier_data_winter = data[data['balance_winter'].notna()]\n",
    "glacier_data_winter.reset_index(drop=True, inplace=True)\n",
    "\n",
    "glacier_data_summer = data[data['balance_summer'].notna()]\n",
    "glacier_data_summer.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8147774-2770-4d40-b5f0-76af1d51c4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test annual: 2845/1065\n",
      "Train/test winter: 2752/1000\n",
      "Train/test summer: 2901/1028\n",
      "All train/test: 8498 / 3093\n",
      "Fraction train/test: 0.7331550340781641 / 0.26684496592183593\n",
      "Total entries: 11591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kasj\\AppData\\Local\\Temp\\ipykernel_18052\\1542360467.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_annual['n_months']=12\n",
      "C:\\Users\\kasj\\AppData\\Local\\Temp\\ipykernel_18052\\1542360467.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_winter['n_months']=7\n",
      "C:\\Users\\kasj\\AppData\\Local\\Temp\\ipykernel_18052\\1542360467.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_summer['n_months']=5\n",
      "C:\\Users\\kasj\\AppData\\Local\\Temp\\ipykernel_18052\\1542360467.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test_annual['n_months']=12\n",
      "C:\\Users\\kasj\\AppData\\Local\\Temp\\ipykernel_18052\\1542360467.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test_winter['n_months']=7\n",
      "C:\\Users\\kasj\\AppData\\Local\\Temp\\ipykernel_18052\\1542360467.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test_summer['n_months']=5\n"
     ]
    }
   ],
   "source": [
    "test_glaciers = [54, 703, 941, 1135, 1280, 2085, 2320, 2478, 2768, 2769, 3133, 3137, 3138, 3141]\n",
    "\n",
    "# Get test dataset for each of annual, winter and summer mass balance\n",
    "df_test_annual = glacier_data_annual[glacier_data_annual['BREID'].isin(test_glaciers)]\n",
    "df_test_winter = glacier_data_winter[glacier_data_winter['BREID'].isin(test_glaciers)]\n",
    "df_test_summer = glacier_data_summer[glacier_data_summer['BREID'].isin(test_glaciers)]\n",
    "# 54 has 189 points\n",
    "# 703 has 30 points\n",
    "# 941 has 70 points\n",
    "# 1280 has 71 points\n",
    "# 2320 has 83 points\n",
    "# 2478 has 89 points\n",
    "# 2769 has 121 points\n",
    "# 3133 has 38 points\n",
    "# 3137 has 65 points\n",
    "# 3138 has 6 points\n",
    "# 3141 has 72 points\n",
    "\n",
    "# Get training dataset for each of annual, winter and summer mass balance\n",
    "df_train_annual = glacier_data_annual[~glacier_data_annual['BREID'].isin(test_glaciers)]\n",
    "df_train_winter = glacier_data_winter[~glacier_data_winter['BREID'].isin(test_glaciers)]\n",
    "df_train_summer = glacier_data_summer[~glacier_data_summer['BREID'].isin(test_glaciers)]\n",
    "\n",
    "# Add number of months to each dataframe\n",
    "df_train_annual['n_months']=12\n",
    "df_train_winter['n_months']=7\n",
    "df_train_summer['n_months']=5\n",
    "df_test_annual['n_months']=12\n",
    "df_test_winter['n_months']=7\n",
    "df_test_summer['n_months']=5\n",
    "\n",
    "print(f'Train/test annual: {len(df_train_annual)}/{len(df_test_annual)}')\n",
    "print(f'Train/test winter: {len(df_train_winter)}/{len(df_test_winter)}')\n",
    "print(f'Train/test summer: {len(df_train_summer)}/{len(df_test_summer)}')\n",
    "print(f'All train/test: {len(df_train_annual)+len(df_train_winter)+len(df_train_summer)} / {len(df_test_annual)+len(df_test_winter)+len(df_test_summer)}')\n",
    "print(f'Fraction train/test: {(len(df_train_annual)+len(df_train_winter)+len(df_train_summer)) / (len(df_test_annual)+len(df_test_winter)+len(df_test_summer)+len(df_train_annual)+len(df_train_winter)+len(df_train_summer))} / {(len(df_test_annual)+len(df_test_winter)+len(df_test_summer)) /(len(df_test_annual)+len(df_test_winter)+len(df_test_summer) + len(df_train_annual)+len(df_train_winter)+len(df_train_summer))}')\n",
    "print(f'Total entries: {len(df_test_annual)+len(df_test_winter)+len(df_test_summer) + len(df_train_annual)+len(df_train_winter)+len(df_train_summer)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38050830-787a-4e5b-80ad-cb63949a4863",
   "metadata": {},
   "source": [
    "### 1.1 Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24038238-f036-4caa-8262-cbfe7a2828b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns to drop\n",
    "cols = ['RGIID','GLIMSID','utm_zone','utm_east_approx','utm_north_approx','altitude_approx',\n",
    "        'location_description','location_id','stake_no','utm_east','utm_north','dt_prev_year_min_date','dt_curr_year_max_date',\n",
    "        'dt_curr_year_min_date','stake_remark','flag_correction','approx_loc','approx_altitude',\n",
    "        'diff_north','diff_east','diff_altitude','diff_netto','lat_approx','lon_approx',\n",
    "        'topo','dis_from_border', 'lat','lon', 'slope_factor']\n",
    "\n",
    "snow_depth_m = ['sde_oct','sde_nov','sde_dec','sde_jan','sde_feb','sde_mar','sde_apr','sde_may','sde_jun','sde_jul','sde_aug','sde_sep']\n",
    "snow_density = ['rsn_oct','rsn_nov','rsn_dec','rsn_jan','rsn_feb','rsn_mar','rsn_apr','rsn_may','rsn_jun','rsn_jul','rsn_aug','rsn_sep']\n",
    "evaporation = ['es_oct','es_nov','es_dec','es_jan','es_feb','es_mar','es_apr','es_may','es_jun','es_jul','es_aug','es_sep']\n",
    "snow_cover = ['snowc_oct','snowc_nov','snowc_dec','snowc_jan','snowc_feb','snowc_mar','snowc_apr','snowc_may','snowc_jun','snowc_jul','snowc_aug','snowc_sep']\n",
    "snow_depth_we = ['sd_oct','sd_nov','sd_dec','sd_jan','sd_feb','sd_mar','sd_apr','sd_may','sd_jun','sd_jul','sd_aug','sd_sep']\n",
    "snow_temp = ['tsn_oct','tsn_nov','tsn_dec','tsn_jan','tsn_feb','tsn_mar','tsn_apr','tsn_may','tsn_jun','tsn_jul','tsn_aug','tsn_sep']\n",
    "snow_melt = ['smlt_oct','smlt_nov','smlt_dec','smlt_jan','smlt_feb','smlt_mar','smlt_apr','smlt_may','smlt_jun','smlt_jul','smlt_aug','smlt_sep']\n",
    "snowfall = ['sf_oct','sf_nov','sf_dec','sf_jan','sf_feb','sf_mar','sf_apr','sf_may','sf_jun','sf_jul','sf_aug','sf_sep']\n",
    "snow_albedo = ['asn_oct','asn_nov','asn_dec','asn_jan','asn_feb','asn_mar','asn_apr','asn_may','asn_jun','asn_jul','asn_aug','asn_sep']\n",
    "dewpt_temp = ['d2m_oct','d2m_nov','d2m_dec','d2m_jan','d2m_feb','d2m_mar','d2m_apr','d2m_may','d2m_jun','d2m_jul','d2m_aug','d2m_sep']\n",
    "surface_pressure = ['sp_oct','sp_nov','sp_dec','sp_jan','sp_feb','sp_mar','sp_apr','sp_may','sp_jun','sp_jul','sp_aug','sp_sep']\n",
    "sol_rad_net = ['ssr_oct','ssr_nov','ssr_dec','ssr_jan','ssr_feb','ssr_mar','ssr_apr','ssr_may','ssr_jun','ssr_jul','ssr_aug','ssr_sep']\n",
    "sol_therm_down = ['strd_oct','strd_nov','strd_dec','strd_jan','strd_feb','strd_mar','strd_apr','strd_may','strd_jun','strd_jul','strd_aug','strd_sep']\n",
    "#sol_rad_down = ['ssrd_oct','ssrd_nov','ssrd_dec','ssrd_jan','ssrd_feb','ssrd_mar','ssrd_apr','ssrd_may','ssrd_jun','ssrd_jul','ssrd_aug','ssrd_sep']\n",
    "u_wind = ['u10_oct','u10_nov','u10_dec','u10_jan','u10_feb','u10_mar','u10_apr','u10_may','u10_jun','u10_jul','u10_aug','u10_sep']\n",
    "v_wind = ['v10_oct','v10_nov','v10_dec','v10_jan','v10_feb','v10_mar','v10_apr','v10_may','v10_jun','v10_jul','v10_aug','v10_sep']\n",
    "#f_albedo = ['fal_oct','fal_nov','fal_dec','fal_jan','fal_feb','fal_mar','fal_apr','fal_may','fal_jun','fal_jul','fal_aug','fal_sep']\n",
    "\n",
    "drop_cols = [y for x in [cols, snow_depth_m, snow_density, evaporation, snow_cover, snow_depth_we, snow_temp, snow_melt, snowfall, snow_albedo, \n",
    "                         dewpt_temp, surface_pressure, sol_rad_net, sol_therm_down, u_wind, v_wind] for y in x]\n",
    "\n",
    "# Select features for training\n",
    "df_train_annual_clean = df_train_annual.drop(drop_cols, axis=1)\n",
    "df_train_winter_clean = df_train_winter.drop(drop_cols, axis=1)\n",
    "df_train_summer_clean = df_train_summer.drop(drop_cols, axis=1)\n",
    "df_train_annual_clean = df_train_annual_clean.drop(['balance_winter','balance_summer'], axis=1)\n",
    "df_train_winter_clean = df_train_winter_clean.drop(['balance_netto', 'balance_summer'], axis=1)\n",
    "df_train_summer_clean = df_train_summer_clean.drop(['balance_netto', 'balance_winter'], axis=1)\n",
    "\n",
    "# Rename target columns to same name\n",
    "df_train_annual_clean.rename(columns={'balance_netto' : 'balance'}, inplace=True)\n",
    "df_train_winter_clean.rename(columns={'balance_winter' : 'balance'}, inplace=True)\n",
    "df_train_summer_clean.rename(columns={'balance_summer' : 'balance'}, inplace=True)\n",
    "\n",
    "# df_train_X_... now contains columns of all chosen features and column with annual, winter or summer balance\n",
    "\n",
    "# For summer balance, replace column values in accumulation months with NaN (oct, nov, dec, jan, feb, mar, apr, may)\n",
    "# For winter balance, replace column values in ablation months with NaN (may, jun, jul, aug, sep, oct)\n",
    "var = ['t2m', 'sshf', 'slhf', 'ssrd', 'fal','str', 'tp']\n",
    "mon_summer = ['may', 'jun', 'jul', 'aug', 'sep']\n",
    "mon_winter = ['oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr']\n",
    "\n",
    "for name in var:\n",
    "    for mon in mon_summer:\n",
    "        if mon not in mon_winter:\n",
    "            df_train_winter_clean[name+'_'+mon]= np.nan\n",
    "for name in var:   \n",
    "    for mon in mon_winter:\n",
    "        if mon not in mon_summer:\n",
    "            df_train_summer_clean[name+'_'+mon]= np.nan\n",
    "\n",
    "# Combine all annual, winter and summer data in one dataframe\n",
    "#df_train_all = pd.concat([df_train_annual_clean, df_train_summer_clean, df_train_winter_clean])\n",
    "\n",
    "# Use altitude_diff instead of altitude and altitude_climate\n",
    "df_train_summer_clean['altitude_diff'] = df_train_summer_clean['altitude_climate']-df_train_summer_clean['altitude']\n",
    "df_train_summer_clean = df_train_summer_clean.drop(['altitude','altitude_climate'], axis=1)\n",
    "df_train_winter_clean['altitude_diff'] = df_train_winter_clean['altitude_climate']-df_train_winter_clean['altitude']\n",
    "df_train_winter_clean = df_train_winter_clean.drop(['altitude','altitude_climate'], axis=1)\n",
    "df_train_annual_clean['altitude_diff'] = df_train_annual_clean['altitude_climate']-df_train_annual_clean['altitude']\n",
    "df_train_annual_clean = df_train_annual_clean.drop(['altitude','altitude_climate'], axis=1)\n",
    "\n",
    "df_train_summer_clean = df_train_summer_clean.dropna(axis=1, how='all')\n",
    "df_train_winter_clean = df_train_winter_clean.dropna(axis=1, how='all')\n",
    "df_train_annual_clean = df_train_annual_clean.dropna(axis=1, how='all')\n",
    "\n",
    "n_summer = len(df_train_summer_clean)\n",
    "n_winter = len(df_train_winter_clean)\n",
    "n_annual = len(df_train_annual_clean)\n",
    "\n",
    "df_train_summer_clean.insert(0, 'id', list(range(n_summer)))\n",
    "df_train_winter_clean.insert(0, 'id', list(range(n_summer, n_summer+n_winter)))\n",
    "df_train_annual_clean.insert(0, 'id', list(range(n_summer+n_winter, n_summer+n_winter+n_annual)))\n",
    "\n",
    "# Columns that are not monthly climate variables (identifiers and static variables)\n",
    "id_vars = ['id','BREID', 'year', 'altitude','balance','aspect','slope','altitude_climate','n_months']\n",
    "\n",
    "# Extract the unique variable names and month names from the column names\n",
    "#variables = set(col.split('_')[0] for col in df.columns if col not in id_vars)\n",
    "#months = set(col.split('_')[-1] for col in df.columns if col not in id_vars)\n",
    "variables = ['t2m', 'sshf', 'slhf', 'ssrd', 'fal','str', 'tp']\n",
    "summer_months_order = ['may', 'jun', 'jul', 'aug', 'sep']\n",
    "winter_months_order = ['oct','nov','dec','jan','feb','mar','apr']\n",
    "annual_months_order = ['oct','nov','dec','jan','feb','mar','apr', 'may', 'jun', 'jul', 'aug', 'sep']\n",
    "\n",
    "# Reshape dataframes to monthly resolution\n",
    "df_train_summer_final = reshape_dataset_monthly(df_train_summer_clean, id_vars, variables, summer_months_order)\n",
    "df_train_winter_final = reshape_dataset_monthly(df_train_winter_clean, id_vars, variables, winter_months_order)\n",
    "df_train_annual_final = reshape_dataset_monthly(df_train_annual_clean, id_vars, variables, annual_months_order)\n",
    "\n",
    "# Combine training data in one dataframe\n",
    "df_train_summer_final.reset_index(drop=True, inplace=True)\n",
    "df_train_winter_final.reset_index(drop=True, inplace=True)\n",
    "df_train_annual_final.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data_list = [df_train_summer_final, df_train_winter_final, df_train_annual_final]\n",
    "df_train_final = pd.concat(data_list)\n",
    "df_train_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93cfc7d-c386-4377-8801-84c84d780ba7",
   "metadata": {},
   "source": [
    "### 1.2 Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c93de37-ac10-43d3-96f7-129fe0e0a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test\n",
    "\n",
    "# Select features for training\n",
    "df_test_annual_clean = df_test_annual.drop(drop_cols, axis=1)\n",
    "df_test_winter_clean = df_test_winter.drop(drop_cols, axis=1)\n",
    "df_test_summer_clean = df_test_summer.drop(drop_cols, axis=1)\n",
    "df_test_annual_clean = df_test_annual_clean.drop(['balance_winter','balance_summer'], axis=1)\n",
    "df_test_winter_clean = df_test_winter_clean.drop(['balance_netto', 'balance_summer'], axis=1)\n",
    "df_test_summer_clean = df_test_summer_clean.drop(['balance_netto', 'balance_winter'], axis=1)\n",
    "\n",
    "# Rename target columns to same name\n",
    "df_test_annual_clean.rename(columns={'balance_netto' : 'balance'}, inplace=True)\n",
    "df_test_winter_clean.rename(columns={'balance_winter' : 'balance'}, inplace=True)\n",
    "df_test_summer_clean.rename(columns={'balance_summer' : 'balance'}, inplace=True)\n",
    "\n",
    "# For summer balance, replace column values in accumulation months with NaN (oct, nov, dec, jan, feb, mar, apr, may)\n",
    "# For winter balance, replace column values in ablation months with NaN (may, jun, jul, aug, sep, oct)\n",
    "#var = ['t2m', 'sshf', 'slhf', 'ssrd', 'fal','str', 'tp']\n",
    "#mon_summer = ['may', 'jun', 'jul', 'aug', 'sep']\n",
    "#mon_winter = ['oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may']\n",
    "\n",
    "for name in var:\n",
    "    for mon in mon_summer:\n",
    "        if mon not in mon_winter:\n",
    "            df_test_winter_clean[name+'_'+mon]= np.nan\n",
    "for name in var:   \n",
    "    for mon in mon_winter:\n",
    "        if mon not in mon_summer:\n",
    "            df_test_summer_clean[name+'_'+mon]= np.nan\n",
    "\n",
    "# Combine all annual, winter and summer data in one dataframe\n",
    "#df_test_all = pd.concat([df_test_annual_clean, df_test_summer_clean, df_test_winter_clean])\n",
    "#df_test_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Use altitude_diff instead of altitude and altitude_climate\n",
    "df_test_summer_clean['altitude_diff'] = df_test_summer_clean['altitude_climate']-df_test_summer_clean['altitude']\n",
    "df_test_summer_clean = df_test_summer_clean.drop(['altitude','altitude_climate'], axis=1)\n",
    "df_test_winter_clean['altitude_diff'] = df_test_winter_clean['altitude_climate']-df_test_winter_clean['altitude']\n",
    "df_test_winter_clean = df_test_winter_clean.drop(['altitude','altitude_climate'], axis=1)\n",
    "df_test_annual_clean['altitude_diff'] = df_test_annual_clean['altitude_climate']-df_test_annual_clean['altitude']\n",
    "df_test_annual_clean = df_test_annual_clean.drop(['altitude','altitude_climate'], axis=1)\n",
    "\n",
    "df_test_summer_clean = df_test_summer_clean.dropna(axis=1, how='all')\n",
    "df_test_winter_clean = df_test_winter_clean.dropna(axis=1, how='all')\n",
    "df_test_annual_clean = df_test_annual_clean.dropna(axis=1, how='all')\n",
    "\n",
    "n_summer = len(df_test_summer_clean)\n",
    "n_winter = len(df_test_winter_clean)\n",
    "n_annual = len(df_test_annual_clean)\n",
    "\n",
    "df_test_summer_clean.insert(0, 'id', list(range(n_summer)))\n",
    "df_test_winter_clean.insert(0, 'id', list(range(n_summer, n_summer+n_winter)))\n",
    "df_test_annual_clean.insert(0, 'id', list(range(n_summer+n_winter, n_summer+n_winter+n_annual)))\n",
    "\n",
    "# Columns that are not monthly climate variables (identifiers and static variables)\n",
    "#id_vars = ['id','BREID', 'year', 'altitude','balance','aspect','slope','altitude_climate','n_months']\n",
    "\n",
    "# Extract the unique variable names and month names from the column names\n",
    "#variables = set(col.split('_')[0] for col in df.columns if col not in id_vars)\n",
    "#months = set(col.split('_')[-1] for col in df.columns if col not in id_vars)\n",
    "#variables = ['t2m', 'sshf', 'slhf', 'ssrd', 'fal','str', 'tp']\n",
    "#summer_months_order = ['may', 'jun', 'jul', 'aug', 'sep']\n",
    "#winter_months_order = ['oct','nov','dec','jan','feb','mar','apr']\n",
    "#annual_months_order = ['oct','nov','dec','jan','feb','mar','apr', 'may', 'jun', 'jul', 'aug', 'sep']\n",
    "\n",
    "# Reshape datasets to monthly resolution\n",
    "df_test_summer_final = reshape_dataset_monthly(df_test_summer_clean, id_vars, variables, summer_months_order)\n",
    "df_test_winter_final = reshape_dataset_monthly(df_test_winter_clean, id_vars, variables, winter_months_order)\n",
    "df_test_annual_final = reshape_dataset_monthly(df_test_annual_clean, id_vars, variables, annual_months_order)\n",
    "\n",
    "# Combine training data in one dataframe\n",
    "df_test_summer_final.reset_index(drop=True, inplace=True)\n",
    "df_test_winter_final.reset_index(drop=True, inplace=True)\n",
    "df_test_annual_final.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data_list = [df_test_summer_final, df_test_winter_final, df_test_annual_final]\n",
    "df_test_final = pd.concat(data_list)\n",
    "df_test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8e7ef-a16d-4e6f-a7c6-11ec475ead8d",
   "metadata": {},
   "source": [
    "## 2 Split training data for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faaf3313-382f-460c-aa95-8558bb2b4c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training\n",
    "df_train_X_reduce = df_train_final.drop(['balance','year','BREID'], axis=1)\n",
    "\n",
    "# Move id and n_months to the end of the dataframe (these are to be used as metadata)\n",
    "df_train_X = df_train_X_reduce[[c for c in df_train_X_reduce if c not in ['id','n_months','month']] + ['id','n_months','month']]\n",
    "\n",
    "# Select labels for training\n",
    "df_train_y = df_train_final[['balance']]\n",
    "\n",
    "# Get arrays of features+metadata and targets\n",
    "X_train, y_train = df_train_X.values, df_train_y.values\n",
    "\n",
    "# Get glacier IDs from training dataset (in the order of which they appear in training dataset).\n",
    "# gp_s is an array with shape equal to the shape of X_train_s and y_train_s.\n",
    "gp_s = np.array(df_train_final['id'].values)\n",
    "\n",
    "# Use five folds\n",
    "group_kf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Split into folds according to group by glacier ID.\n",
    "# For each unique glacier ID, indices in gp_s indicate which rows in X_train_s and y_train_s belong to the glacier.\n",
    "splits_s = list(group_kf.split(X_train, y_train, gp_s))\n",
    "\n",
    "#print('Train, fold 0: ', np.unique(gp_s[splits_s[0][0]]))\n",
    "#print('Validation, fold 0: ', np.unique(gp_s[splits_s[0][1]]))\n",
    "#print('Train, fold 1: ', np.unique(gp_s[splits_s[1][0]]))\n",
    "#print('Validation, fold 1: ', np.unique(gp_s[splits_s[1][1]]))\n",
    "#print('Train, fold 2: ', np.unique(gp_s[splits_s[2][0]]))\n",
    "#print('Validation, fold 2: ', np.unique(gp_s[splits_s[2][1]]))\n",
    "#print('Train, fold 3: ', np.unique(gp_s[splits_s[3][0]]))\n",
    "#print('Validation, fold 3: ', np.unique(gp_s[splits_s[3][1]]))\n",
    "#print('Train, fold 4: ', np.unique(gp_s[splits_s[4][0]]))\n",
    "#print('Validation, fold 4: ', np.unique(gp_s[splits_s[4][1]]))\n",
    "print(len(gp_s))\n",
    "print(y_train.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc333a4e-e148-4798-b452-c3414a6ee089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [    0     1     2 ... 67894 67895 67896] TEST: [   15    16    17 ... 67906 67907 67908]\n",
      "shape(train): (54329,) test: (13580,)\n",
      "TRAIN: [    0     1     2 ... 67906 67907 67908] TEST: [   10    11    12 ... 67858 67859 67860]\n",
      "shape(train): (54329,) test: (13580,)\n",
      "TRAIN: [    0     1     2 ... 67906 67907 67908] TEST: [    5     6     7 ... 67846 67847 67848]\n",
      "shape(train): (54326,) test: (13583,)\n",
      "TRAIN: [    0     1     2 ... 67906 67907 67908] TEST: [   25    26    27 ... 67834 67835 67836]\n",
      "shape(train): (54326,) test: (13583,)\n",
      "TRAIN: [    5     6     7 ... 67906 67907 67908] TEST: [    0     1     2 ... 67894 67895 67896]\n",
      "shape(train): (54326,) test: (13583,)\n"
     ]
    }
   ],
   "source": [
    "# Check fold indices for training/validation data\n",
    "fold_indices = []\n",
    "\n",
    "for train_index, test_index in group_kf.split(X_train, y_train, gp_s):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(\"shape(train):\", train_index.shape, \"test:\", test_index.shape)\n",
    "    fold_indices.append((train_index, test_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21ffba22-7104-4e20-b486-8d101d879146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for test\n",
    "df_test_X_reduce = df_test_final.drop(['balance','year','BREID'], axis=1)\n",
    "\n",
    "# Move id and n_months to the end of the dataframe (these are to be used as metadata)\n",
    "df_test_X = df_test_X_reduce[[c for c in df_test_X_reduce if c not in ['id','n_months','month']] + ['id','n_months','month']]\n",
    "\n",
    "# Select labels for test\n",
    "df_test_y = df_test_final[['balance']]\n",
    "\n",
    "# Get arrays of features+metadata and targets\n",
    "X_test, y_test = df_test_X.values, df_test_y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3c80b-ba6c-4eb2-bbb1-eac572358daa",
   "metadata": {},
   "source": [
    "## 3 Define loss function and helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a02fdab-454e-4fdc-a575-adf21ede99bc",
   "metadata": {},
   "source": [
    "#### Custom loss/objective function scikit learn api with metadata\n",
    "\n",
    "To be used with custom XGBRegressor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ad439d-8dea-41f3-8bcb-a7489883e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom objective function scikit learn api with metadata, to be used with custom XGBRegressor class\n",
    "\n",
    "def custom_mse_metadata(y_true, y_pred, metadata):\n",
    "    \"\"\"\n",
    "    Custom Mean Squared Error (MSE) objective function for evaluating monthly predictions with respect to \n",
    "    seasonally or annually aggregated observations.\n",
    "    \n",
    "    For use in cases where predictions are done on a monthly time scale and need to be aggregated to be\n",
    "    compared with the true aggregated seasonal or annual value. Aggregations are performed according to a\n",
    "    unique ID provided by metadata. The function computes gradients and hessians \n",
    "    used in gradient boosting methods, specifically for use with the XGBoost library's custom objective \n",
    "    capabilities.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : numpy.ndarray\n",
    "        True (seasonally or annually aggregated) values for each instance. For a unique ID, \n",
    "        values are repeated n_months times across the group, e.g. the annual mass balance for a group\n",
    "        of 12 monthly predictions with the same unique ID is repeated 12 times. Before calculating the \n",
    "        loss, the mean over the n unique IDs is taken.\n",
    "    \n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted monthly values. These predictions will be aggregated according to the \n",
    "        unique ID before calculating the loss, e.g. 12 monthly predictions with the same unique ID is\n",
    "        aggregated for evaluation against the true annual value.\n",
    "    \n",
    "    metadata : numpy.ndarray\n",
    "        An ND numpy array containing metadata for each monthly prediction. The first column is mandatory \n",
    "        and represents the ID of the aggregated group to which each instance belongs. Each group identified \n",
    "        by a unique ID will be aggregated together for the loss calculation. The following columns in the \n",
    "        metadata can include additional information for each instance that may be useful for tracking or further \n",
    "        processing but are not used in the loss calculation, e.g. number of months to be aggregated or the name \n",
    "        of the month.\n",
    "        \n",
    "        ID (column 0): An integer that uniquely identifies the group which the instance belongs to.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    gradients : numpy.ndarray\n",
    "        The gradient of the loss with respect to the predictions y_pred. This array has the same shape \n",
    "        as y_pred.\n",
    "    \n",
    "    hessians : numpy.ndarray\n",
    "        The second derivative (hessian) of the loss with respect to the predictions y_pred. For MSE loss, \n",
    "        the hessian is constant and thus this array is filled with ones, having the same shape as y_pred.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize empty arrays for gradient and hessian\n",
    "    gradients = np.zeros_like(y_pred)\n",
    "    hessians = np.ones_like(y_pred) # Ones in case of mse\n",
    "    \n",
    "    # Unique aggregation groups based on the aggregation ID\n",
    "    unique_ids = np.unique(metadata[:, 0])\n",
    "    \n",
    "    # Loop over each unique ID to aggregate accordingly\n",
    "    for uid in unique_ids:\n",
    "        # Find indexes for the current aggregation group\n",
    "        indexes = metadata[:, 0] == uid\n",
    "        \n",
    "        # Aggregate y_pred for the current group\n",
    "        y_pred_agg = np.sum(y_pred[indexes])\n",
    "        \n",
    "        # True value is the same repeated value for the group, so we can use the mean\n",
    "        y_true_mean = np.mean(y_true[indexes])\n",
    "        \n",
    "        # Compute gradients for the group based on the aggregated prediction\n",
    "        gradient = y_pred_agg - y_true_mean\n",
    "        gradients[indexes] = gradient\n",
    "\n",
    "    return gradients, hessians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371dad88-b159-42f7-9df9-a777486bd3ac",
   "metadata": {},
   "source": [
    "#### Custom estimator class based on XGBRegressor\n",
    "\n",
    "Inherits from xgboost.XGBRegressor \n",
    "\n",
    "Note/to-do: Modify def __init__(self,..) to retrieve kwarg for number of columns of metadata (n_metadata) with default n_metadata=None, to allow for flexibility in the number of metadata columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849ac08-812b-4118-af16-b45bc099a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "class CustomXGBRegressor(XGBRegressor):\n",
    "    \"\"\"\n",
    "    CustomXGBRegressor is an extension of the XGBoost regressor that incorporates additional metadata into the learning process. The estimator\n",
    "    is tailored to handle training datasets where the last three columns are metadata rather than features.\n",
    "    \n",
    "    The metadata is utilized in a custom mean squared error function. This function calculates gradients and hessians incorporating metadata, \n",
    "    allowing the model to learn from both standard feature data and additional information provided as metadata.\n",
    "    \n",
    "    The custom objective closure captures metadata along with the target values and predicted values to compute the gradients and hessians needed\n",
    "    for the XGBoost training process.\n",
    "    \n",
    "    Parameters inherited from XGBRegressor are customizable and additional parameters can be passed via kwargs, which will be handled by the\n",
    "    XGBRegressor's __init__ method.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> model = CustomXGBRegressor(n_estimators=500, learning_rate=0.05)\n",
    "    >>> model.fit(X_train, y_train)  # X_train includes metadata as the last 3 columns\n",
    "    >>> predictions = model.predict(X_test)  # X_test includes metadata as the last 3 columns\n",
    "    \n",
    "    Note: CustomXGBRegressor requires a custom MSE function, `custom_mse_metadata`, which computes the gradient and hessian using additional metadata.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metadata_shape=3, **kwargs):\n",
    "        self.metadata_shape = metadata_shape\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def fit(self, X, y, **fit_params):\n",
    "        # Split features from metadata\n",
    "        metadata, features = X[:, -self.metadata_shape:], X[:, :-self.metadata_shape]\n",
    "\n",
    "        # Define closure that captures metadata for use in custom objective\n",
    "        def custom_objective(y_true, y_pred):\n",
    "            return custom_mse_metadata(y_true, y_pred, metadata)\n",
    "\n",
    "        # Set custom objective\n",
    "        self.set_params(objective=custom_objective)\n",
    "\n",
    "        # Call fit method from parent class (XGBRegressor)\n",
    "        super().fit(features, y, **fit_params)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check if the model is fitted\n",
    "        check_is_fitted(self)\n",
    "        \n",
    "        features = X[:, :-self.metadata_shape]\n",
    "        \n",
    "        return super().predict(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319677ff-784f-4116-9c98-07377deaa5e9",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45bac67-7322-42d0-bc80-3397cec70cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get true values (means) and predicted values (aggregates)\n",
    "\n",
    "def get_ytrue_y_pred_agg(y_true, y_pred, X):\n",
    "    # Extract the metadata\n",
    "    metadata = X[:, -3:]  # Assuming last three columns are the metadata\n",
    "    unique_ids = np.unique(metadata[:, 0]) # Assuming ID is the first column\n",
    "    y_pred_agg_all = []\n",
    "    y_true_mean_all = []\n",
    "    \n",
    "    # Loop over each unique ID to calculate MSE\n",
    "    for uid in unique_ids:\n",
    "        # Indexes for the current ID\n",
    "        indexes = metadata[:, 0] == uid\n",
    "        # Aggregate y_pred for the current ID\n",
    "        y_pred_agg = np.sum(y_pred[indexes])\n",
    "        y_pred_agg_all.append(y_pred_agg)\n",
    "        # True value is the mean of true values for the group\n",
    "        y_true_mean = np.mean(y_true[indexes])\n",
    "        y_true_mean_all.append(y_true_mean)\n",
    "\n",
    "    y_pred_agg_all_arr = np.array(y_pred_agg_all)\n",
    "    y_true_mean_all_arr = np.array(y_true_mean_all)\n",
    "    \n",
    "    return y_true_mean_all_arr, y_pred_agg_all_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761aab6-311d-4d2a-9a59-f19d9a5f32ad",
   "metadata": {},
   "source": [
    "## 4 Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955df62-2e7b-4fc5-b573-8d131731dc0c",
   "metadata": {},
   "source": [
    "### 4.1 Model testing: Train model on one fold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d73b8a77-c702-4dcf-add4-f5a959f78b92",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Select fold 0 as training data and fold 1 as validation data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train_crop \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m[fold_indices[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],:]\n\u001b[0;32m      3\u001b[0m X_val_crop \u001b[38;5;241m=\u001b[39m X_train[fold_indices[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m],:]\n\u001b[0;32m      4\u001b[0m y_train_crop \u001b[38;5;241m=\u001b[39m y_train[fold_indices[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Select fold 0 as training data and fold 1 as validation data\n",
    "X_train_crop = X_train[fold_indices[0][0],:]\n",
    "X_val_crop = X_train[fold_indices[0][1],:]\n",
    "y_train_crop = y_train[fold_indices[0][0]]\n",
    "y_val_crop = y_train[fold_indices[0][1]]\n",
    "\n",
    "# Check folds\n",
    "print(X_train_crop.shape)\n",
    "print(X_val_crop.shape)\n",
    "print(y_train_crop.shape)\n",
    "print(y_val_crop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d3a95-75fe-4a67-bd98-695a2ba9180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom XGBRegressor object\n",
    "xgb_model = CustomXGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, gamma=0)\n",
    "\n",
    "# Fit model for fold 0\n",
    "xgb_model.fit(X_train_crop, y_train_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd85c6-b07e-4f22-9fa7-625b9fb61978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the validation data\n",
    "predictions = xgb_model.predict(X_val_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce3800e-e8b2-4228-9f31-627991f4879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation true values and predictions\n",
    "y_true_mean, y_pred_agg = get_ytrue_y_pred_agg(y_val_crop,predictions,X_val_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc18bc8-1441-4d0c-9f70-b91bbd94c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs true values for fold 1\n",
    "plot_prediction(y_true_mean, y_pred_agg, data_type='val fold1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3205c57b-e3e0-4a83-ab8f-c05b046e2888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "test_pred = xgb_model_split.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b6af2e-6657-470e-b4fb-2d4bd07272e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test true values and predictions\n",
    "y_test_mean, y_test_pred_agg = get_ytrue_y_pred_agg(y_test,test_pred,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b8542-c0b4-411b-85ee-35c7b61a4a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs true values for test\n",
    "plot_prediction(y_test_mean, y_test_pred_agg, data_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "568998cc-2985-46ea-a83f-e5f30a7b00c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54329, 13)\n",
      "(13580, 13)\n",
      "(54329, 1)\n",
      "(13580, 1)\n"
     ]
    }
   ],
   "source": [
    "# Plot predicted mass balance distribution for each month\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "months = X_val_crop[:,-1]\n",
    "\n",
    "# Define the month order\n",
    "month_order = ['oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep']\n",
    "\n",
    "bin_width = 0.1 \n",
    "\n",
    "min_pred = predictions.min()\n",
    "max_pred = predictions.max()\n",
    "\n",
    "# Create bins with the specified width from the global min to max prediction\n",
    "bins = np.arange(min_pred, max_pred + bin_width, bin_width)\n",
    "\n",
    "fig, ax = plt.subplots(len(month_order), 1, figsize=(10, 2*len(month_order)), sharex = True, sharey=True)\n",
    "\n",
    "for i, month in enumerate(month_order):\n",
    "    month_predictions = predictions[months == month]\n",
    "    \n",
    "    ax[i].hist(month_predictions, bins=bins, alpha=0.75, label=f'{month.capitalize()} Predictions')\n",
    "\n",
    "    ax[i].xaxis.set_tick_params(which='both', labelbottom=True)\n",
    "    \n",
    "    #ax[i].set_title(f'{month.capitalize()} distribution')\n",
    "    ax[i].set_ylabel('Frequency')\n",
    "    ax[i].legend(loc='upper left')\n",
    "\n",
    "ax[-1].set_xlabel('Predicted Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d31c8c-6c09-4cd1-b44c-a2ba6f7a382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted mass balance gradient for each month\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "months = X_val_crop[:,-1]\n",
    "elevation = X_val_crop[:,0]\n",
    "\n",
    "# Define month order\n",
    "month_order = ['oct', 'nov', 'dec', 'jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep']\n",
    "colors = plt.cm.rainbow(np.linspace(0, 1, len(month_order)))\n",
    "\n",
    "# Map month to color\n",
    "month_to_color = dict(zip(month_order, colors))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "for month in month_order:\n",
    "    idx = months == month\n",
    "    ax.scatter(predictions[idx], elevation[idx], label=month.capitalize(), alpha=0.6, c=[month_to_color[month]])\n",
    "\n",
    "ax.set_xlabel('Predicted mass balance (m w.e.)')\n",
    "ax.set_ylabel('Elevation (m a.s.l.')\n",
    "ax.set_title('Predicted mb gradients')\n",
    "ax.legend()\n",
    "\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc742e7-1ea3-4298-bd56-600e5e6a9c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
